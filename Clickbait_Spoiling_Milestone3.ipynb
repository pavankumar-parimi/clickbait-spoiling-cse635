{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wUZzyTeAgd00",
        "-_VTFFPb67kA",
        "CgLA4uAT-w60"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18ad715675774f56a3cf89e4c5d2ddab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f474458377f427e9c57549094b73d41",
              "IPY_MODEL_8f7c6e45ecc44ff5afa0304160177e0d",
              "IPY_MODEL_3d0dd7d2959c40849476b531684e2503"
            ],
            "layout": "IPY_MODEL_3423c023731441d4a24f974502b5ceee"
          }
        },
        "9f474458377f427e9c57549094b73d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a88341c7fc4e828ab24e36786dabe9",
            "placeholder": "​",
            "style": "IPY_MODEL_73a29adb8dc646c7aef07ad87402f212",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "8f7c6e45ecc44ff5afa0304160177e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dd0e9e411324139a333cda9043520d2",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7eb100008a86413394f4e007ac120f7f",
            "value": 501200538
          }
        },
        "3d0dd7d2959c40849476b531684e2503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ec79183aaa4f7f9c59f5b68e65cb3c",
            "placeholder": "​",
            "style": "IPY_MODEL_66b7d643ddd24fbd97c1da7132c2fc8d",
            "value": " 501M/501M [00:00&lt;00:00, 579MB/s]"
          }
        },
        "3423c023731441d4a24f974502b5ceee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a88341c7fc4e828ab24e36786dabe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a29adb8dc646c7aef07ad87402f212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dd0e9e411324139a333cda9043520d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eb100008a86413394f4e007ac120f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48ec79183aaa4f7f9c59f5b68e65cb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b7d643ddd24fbd97c1da7132c2fc8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "Y3qI4jojTzHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Based Installations\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "XZ6J3eFSTyyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Based Installations\n",
        "\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pytorch-lightning==1.2.7 \n",
        "!pip install torchtext==0.6 torch\n",
        "!pip install evaluate\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "5isx8pFNaGU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Data Collection"
      ],
      "metadata": {
        "id": "rBdcIOvcS3B0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction Imports"
      ],
      "metadata": {
        "id": "ErKdNx-_Zeck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, f1_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "9EJo18h-S5hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "XU6Ao4rhXYLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Based Imports"
      ],
      "metadata": {
        "id": "-WWE8GvkZmy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import  Dataset,DataLoader"
      ],
      "metadata": {
        "id": "_cdEnKCPZvvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Reading --> Train & Test Data"
      ],
      "metadata": {
        "id": "QgBO1T5VZyqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_json(\"train.jsonl\", lines = True)\n",
        "test_df = pd.read_json(\"validation.jsonl\", lines = True)"
      ],
      "metadata": {
        "id": "wqoC5pgsXiat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Data Shape: {train_df.shape}\")\n",
        "print(f\"Test Data Shape: {test_df.shape}\")"
      ],
      "metadata": {
        "id": "P0sJdPkAYBQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning & Data Processing"
      ],
      "metadata": {
        "id": "qNj0k11lS6IP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Based Preprocessing"
      ],
      "metadata": {
        "id": "F5xl-_42lVZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data_for_feature_based_models(text):\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "7USMSyuzTBZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_string(spoiler_type):\n",
        "  if spoiler_type[0] == \"passage\":\n",
        "    return 0\n",
        "  elif spoiler_type[0] == \"phrase\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "CD8-OcoFeM5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_df = train_df[[\"targetTitle\", \"targetParagraphs\", \"tags\"]]\n",
        "f_test_df = test_df[[\"targetTitle\", \"targetParagraphs\", \"tags\"]]"
      ],
      "metadata": {
        "id": "NkD1bSZedQBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_df[\"targetParagraphs\"] = f_train_df[\"targetParagraphs\"].apply(format_data_for_feature_based_models)\n",
        "f_test_df[\"targetParagraphs\"] = f_test_df[\"targetParagraphs\"].apply(format_data_for_feature_based_models)"
      ],
      "metadata": {
        "id": "1DGT3BB7dd4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_df[\"tags\"] = f_train_df['tags'].apply(list_to_string)\n",
        "f_test_df[\"tags\"] = f_test_df['tags'].apply(list_to_string)"
      ],
      "metadata": {
        "id": "YlmEtRd9eqC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_df.head(3)"
      ],
      "metadata": {
        "id": "Tah5QquMgArY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_df.iloc[0]['targetParagraphs']"
      ],
      "metadata": {
        "id": "D2lMiZyggMHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_test_df.head(3)"
      ],
      "metadata": {
        "id": "oj9znfaKgCws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_test_df.iloc[0]['targetParagraphs']"
      ],
      "metadata": {
        "id": "WOvLFoVLgVD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_df.rename(columns={\"targetTitle\":\"Post\", \"targetParagraphs\":\"Content\", \"tags\":\"Spoiler_Type\"}, inplace = True)\n",
        "f_test_df.rename(columns={\"targetTitle\":\"Post\", \"targetParagraphs\":\"Content\", \"tags\":\"Spoiler_Type\"}, inplace = True)"
      ],
      "metadata": {
        "id": "B0-id_bxhJoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "iKGyLQhwhYa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.columns"
      ],
      "metadata": {
        "id": "60GcpSNPhdVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_df.columns"
      ],
      "metadata": {
        "id": "-KjpjtT7hfRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_test_df.columns"
      ],
      "metadata": {
        "id": "MMDxrUxPhi0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(train_data, test_data, model=\"bag_of_words\"):\n",
        "  if model == \"bag_of_words\":\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(train_data[\"Content\"])\n",
        "    return vectorizer.transform(train_data[\"Content\"]), vectorizer.transform(test_data[\"Content\"])\n",
        "  elif model == \"tf_idf_model\":\n",
        "    vectorizer = TfidfVectorizer(stop_words = \"english\", max_features=10000)\n",
        "    return vectorizer.fit_transform(train_data[\"Content\"]), vectorizer.transform(test_data[\"Content\"])    \n",
        "  elif model == \"****\":\n",
        "    pass"
      ],
      "metadata": {
        "id": "sW6OKH7Vlbbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "naphO68uTCQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Based Classification"
      ],
      "metadata": {
        "id": "wUZzyTeAgd00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "rYBEzdRApJrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bag Of Words Model"
      ],
      "metadata": {
        "id": "IVBa2u9CpSjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Bag Of Words Model\n",
        "train_x_bag, test_x_bag = get_features(f_train_df, f_test_df, \"bag_of_words\")\n",
        "\n",
        "# 1. Multi-Class\n",
        "model = LogisticRegression(max_iter = 1000, multi_class = \"multinomial\", class_weight = \"balanced\")\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_multi_bag = model.predict(test_x_bag)\n",
        "\n",
        "print(\"Y_test shape:\", f_test_df['Spoiler_Type'].shape)\n",
        "print(\"Predict_test_multi_bag:\", predict_test_multi_bag.shape)\n",
        "  # Evaluation Metrics:\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score Multi Class\")\n",
        "print(\"F1-Score for Multi Class Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_multi_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for Multi Class Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_multi_bag)))\n",
        "\n",
        "# 2. OneVsRest\n",
        "model = OneVsRestClassifier(LogisticRegression(max_iter = 1000, class_weight = \"balanced\"))\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovr_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs Rest\")\n",
        "print(\"F1-Score for One Vs Rest Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs Rest Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag)))\n",
        "\n",
        "\n",
        "# 3. OneVsOne\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter = 1000, class_weight = \"balanced\"))\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovo_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs One\")\n",
        "print(\"F1-Score for One Vs One Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs One Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag)))\n"
      ],
      "metadata": {
        "id": "l8O5bxV-lROi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF Model"
      ],
      "metadata": {
        "id": "QHuQNoJHpVZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using TF-IDF Model\n",
        "train_x_bag, test_x_bag = get_features(f_train_df, f_test_df, \"tf_idf_model\")\n",
        "\n",
        "# 1. Multi-Class\n",
        "model = LogisticRegression(max_iter = 1000, multi_class = \"multinomial\", class_weight = \"balanced\")\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_multi_bag = model.predict(test_x_bag)\n",
        "\n",
        "print(\"Y_test shape:\", f_test_df['Spoiler_Type'].shape)\n",
        "print(\"Predict_test_multi_bag:\", predict_test_multi_bag.shape)\n",
        "  # Evaluation Metrics:\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score Multi Class\")\n",
        "print(\"F1-Score for Multi Class Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_multi_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for Multi Class Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_multi_bag)))\n",
        "\n",
        "# 2. OneVsRest\n",
        "model = OneVsRestClassifier(LogisticRegression(max_iter = 1000, class_weight = \"balanced\"))\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovr_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs Rest\")\n",
        "print(\"F1-Score for One Vs Rest Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs Rest Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag)))\n",
        "\n",
        "\n",
        "# 3. OneVsOne\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter = 1000, class_weight = \"balanced\"))\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovo_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs One\")\n",
        "print(\"F1-Score for One Vs One Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs One Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag)))\n"
      ],
      "metadata": {
        "id": "Onb47jsZpaP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machines"
      ],
      "metadata": {
        "id": "-_VTFFPb67kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag-Of-Model"
      ],
      "metadata": {
        "id": "8tEj1Gow7BPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Bag Of Words Model\n",
        "train_x_bag, test_x_bag = get_features(f_train_df, f_test_df, \"bag_of_words\")\n",
        "\n",
        "# 1. Multi-Class\n",
        "model = SVC(max_iter=1000, class_weight=\"balanced\", decision_function_shape=\"ovr\")\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovr_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs Rest Class\")\n",
        "print(\"F1-Score for One Vs Rest Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs Rest Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag)))\n",
        "\n",
        "\n",
        "# 3. OneVsOne\n",
        "\n",
        "model = SVC(max_iter=1000, class_weight=\"balanced\", decision_function_shape=\"ovo\")\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovo_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs One\")\n",
        "print(\"F1-Score for One Vs One Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs One Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag)))\n"
      ],
      "metadata": {
        "id": "_4KHv5xc7P6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Model"
      ],
      "metadata": {
        "id": "0eN4drOS7GZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using TF-IDF Model\n",
        "train_x_bag, test_x_bag = get_features(f_train_df, f_test_df, \"tf_idf_model\")\n",
        "\n",
        "# 1. Multi-Class\n",
        "model = SVC(max_iter=1000, class_weight=\"balanced\", decision_function_shape=\"ovr\")\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovr_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs Rest Class\")\n",
        "print(\"F1-Score for One Vs Rest Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs Rest Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag)))\n",
        "\n",
        "\n",
        "# 3. OneVsOne\n",
        "\n",
        "model = SVC(max_iter=1000, class_weight=\"balanced\", decision_function_shape=\"ovo\")\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovo_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs One\")\n",
        "print(\"F1-Score for One Vs One Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs One Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag)))\n"
      ],
      "metadata": {
        "id": "CFxTRbvj7QXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "CgLA4uAT-w60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag Of Words Model"
      ],
      "metadata": {
        "id": "maZfCspJ-0JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Bag Of Words Model\n",
        "train_x_bag, test_x_bag = get_features(f_train_df, f_test_df, \"bag_of_words\")\n",
        "\n",
        "# 1. Multi-Class\n",
        "model = MultinomialNB()\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_multi_bag = model.predict(test_x_bag)\n",
        "\n",
        "# Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score Multi Class\")\n",
        "print(\"F1-Score for Multi Class Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_multi_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for Multi Class Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_multi_bag)))\n",
        "\n",
        "# 2. OneVsRest\n",
        "model = OneVsRestClassifier(MultinomialNB())\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovr_bag = model.predict(test_x_bag)\n",
        "\n",
        "# Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs Rest\")\n",
        "print(\"F1-Score for One Vs Rest Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs Rest Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag)))\n",
        "\n",
        "\n",
        "# 3. OneVsOne\n",
        "model = OneVsOneClassifier(MultinomialNB())\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "predict_test_ovo_bag = model.predict(test_x_bag)\n",
        "\n",
        "# Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs One\")\n",
        "print(\"F1-Score for One Vs One Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs One Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag)))\n"
      ],
      "metadata": {
        "id": "MYIvP01s-7dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Model"
      ],
      "metadata": {
        "id": "iiF8aY2S-5Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Bag Of Words Model\n",
        "train_x_bag, test_x_bag = get_features(f_train_df, f_test_df, \"tf_idf_model\")\n",
        "\n",
        "# 1. Multi-Class\n",
        "model = MultinomialNB()\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_multi_bag = model.predict(test_x_bag)\n",
        "\n",
        "# Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score Multi Class\")\n",
        "print(\"F1-Score for Multi Class Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_multi_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for Multi Class Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_multi_bag)))\n",
        "\n",
        "# 2. OneVsRest\n",
        "model = OneVsRestClassifier(MultinomialNB())\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "\n",
        "predict_test_ovr_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs Rest\")\n",
        "print(\"F1-Score for One Vs Rest Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs Rest Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovr_bag)))\n",
        "\n",
        "\n",
        "# 3. OneVsOne\n",
        "model = OneVsOneClassifier(MultinomialNB())\n",
        "model.fit(train_x_bag, f_train_df['Spoiler_Type'])\n",
        "predict_test_ovo_bag = model.predict(test_x_bag)\n",
        "\n",
        "  # Evaluation Metrics:\n",
        "print(\"\\n\")\n",
        "print(\"F1-Score One Vs One\")\n",
        "print(\"F1-Score for One Vs One Test Data:{:.2f}%\".format(f1_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag, average=\"weighted\")))\n",
        "print(\"Accuracy-Score for One Vs One Test Data:{:.2f}%\".format(accuracy_score(f_test_df['Spoiler_Type'], predict_test_ovo_bag)))"
      ],
      "metadata": {
        "id": "x-GuJzs2-8SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Based Classification"
      ],
      "metadata": {
        "id": "BMWlq7Wagisf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roberta Based Classification"
      ],
      "metadata": {
        "id": "0uANkfmbpxVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuYcnGHsxRMh",
        "outputId": "178a745d-71a7-424c-e05f-a71135a4700c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyD9yVjxw5oi",
        "outputId": "9cbbdc88-db02-4ec2-e698-bda6d3025e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "ortNHiIuO0Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_json(\"/content/drive/MyDrive/NLP_dataset/train.jsonl\", lines = True)\n",
        "test_df = pd.read_json(\"/content/drive/MyDrive/NLP_dataset/validation.jsonl\", lines = True)"
      ],
      "metadata": {
        "id": "Lqarr6JFxDGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train data shape:\",train_df.shape)\n",
        "print(\"validation data shape:\",test_df.shape)"
      ],
      "metadata": {
        "id": "258EOZxqgiGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_bkp = train_df.copy()\n",
        "test_df_bkp = test_df.copy()"
      ],
      "metadata": {
        "id": "FyNAYDgA1LSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[[\"postText\", \"targetTitle\", \"targetParagraphs\", \"tags\"]]\n",
        "test_df = test_df[[\"postText\", \"targetTitle\", \"targetParagraphs\", \"tags\"]]"
      ],
      "metadata": {
        "id": "dylaIa0Oq690"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.rename(columns={\"postText\":\"Post\",\"targetTitle\":\"Title\", \"targetParagraphs\":\"Content\", \"tags\":\"Spoiler_Type\"}, inplace = True)\n",
        "# train_df.head(2)\n",
        "test_df.rename(columns={\"postText\":\"Post\",\"targetTitle\":\"Title\", \"targetParagraphs\":\"Content\", \"tags\":\"Spoiler_Type\"}, inplace = True)\n",
        "# test_df.head(2)"
      ],
      "metadata": {
        "id": "b37_n_0WrF65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numeric_labels(spoiler_type):\n",
        "  if spoiler_type[0] == \"passage\":\n",
        "    return 1\n",
        "  elif spoiler_type[0] == \"phrase\":\n",
        "    return 0\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "d9P6AeW3rNEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_string_concat(TgtPara):\n",
        "  res = ' '.join(TgtPara)\n",
        "  return res"
      ],
      "metadata": {
        "id": "uDQ61n8mrPx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"Spoiler_Type\"] = test_df[\"Spoiler_Type\"].apply(numeric_labels)\n",
        "train_df[\"Spoiler_Type\"] = train_df[\"Spoiler_Type\"].apply(numeric_labels)"
      ],
      "metadata": {
        "id": "8VODdeQrrWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"Content\"] = train_df[\"Content\"].apply(list_to_string_concat)\n",
        "test_df[\"Content\"] = test_df[\"Content\"].apply(list_to_string_concat)"
      ],
      "metadata": {
        "id": "H2wZNLf_tFt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"Post\"] = train_df[\"Post\"].apply(list_to_string_concat)\n",
        "test_df[\"Post\"] = test_df[\"Post\"].apply(list_to_string_concat)"
      ],
      "metadata": {
        "id": "sdl4JeVl1xG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.fillna('')\n",
        "test_df = test_df.fillna('')"
      ],
      "metadata": {
        "id": "OVO_YqcVtdGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lower-case:\n",
        "train_df['Post'] = train_df['Post'].str.lower()\n",
        "train_df['Title'] = train_df['Title'].str.lower()\n",
        "train_df['Content'] = train_df['Content'].str.lower()\n",
        "#Lower-case:\n",
        "test_df['Post'] = test_df['Post'].str.lower()\n",
        "test_df['Title'] = test_df['Title'].str.lower()\n",
        "test_df['Content'] = test_df['Content'].str.lower()"
      ],
      "metadata": {
        "id": "av3FNVWEt4zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_multi = train_df.copy()"
      ],
      "metadata": {
        "id": "RyUZvnen18sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_multi = df_multi[df_multi['Spoiler_Type']==2]"
      ],
      "metadata": {
        "id": "EuPVJ0ERd_4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_multi.reset_index(drop=True,inplace=True)"
      ],
      "metadata": {
        "id": "2iQ0ZRBpw2v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# def get_synonym(word):\n",
        "#     \"\"\"\n",
        "#     Get the most suitable synonym for a word\n",
        "#     \"\"\"\n",
        "#     synonyms = []\n",
        "#     for syn in wordnet.synsets(word):\n",
        "#         for lemma in syn.lemmas():\n",
        "#             synonyms.append(lemma.name())\n",
        "    \n",
        "#     # Calculate the similarity scores of all synonyms to the original word\n",
        "#     word_similarities = []\n",
        "#     for synonym in synonyms:\n",
        "#         word_similarities.append((synonym, nlp(word).similarity(nlp(synonym))))\n",
        "    \n",
        "#     # Sort synonyms by their similarity score\n",
        "#     word_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "#     # Select the first synonym if there is at least one with a non-zero similarity score\n",
        "#     for syn in word_similarities:\n",
        "#         if word != syn[0] and syn[1] > 0:\n",
        "#             return syn[0]\n",
        "    \n",
        "#     # Return the original word if there are no suitable synonyms\n",
        "#     return word"
      ],
      "metadata": {
        "id": "AhN3ffMt168r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def change_synonyms(sentence):\n",
        "#   doc = nlp(sentence)\n",
        "#   new_sentence = []\n",
        "#   for token in doc:\n",
        "#       if token.pos_ == \"NOUN\":\n",
        "#           new_sentence.append(get_synonym(token.text))\n",
        "#       else:\n",
        "#           new_sentence.append(token.text)\n",
        "\n",
        "#   altered_sentence = ' '.join(new_sentence)\n",
        "#   return altered_sentence"
      ],
      "metadata": {
        "id": "M5WjgITdAUGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_multi['Post'] = df_multi['Post'].apply(change_synonyms)\n",
        "# df_multi['Title'] = df_multi['Title'].apply(change_synonyms)\n",
        "# df_multi['Content'] = df_multi['Content'].apply(change_synonyms)"
      ],
      "metadata": {
        "id": "ablQqy8CAWtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the dataframe to a CSV file with headers\n",
        "df_multi.to_csv('multi_part_train_data_new.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "_jFYu_RvraW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lower-case:\n",
        "df_multi['Post'] = df_multi['Post'].str.lower()\n",
        "df_multi['Title'] = df_multi['Title'].str.lower()\n",
        "df_multi['Content'] = df_multi['Content'].str.lower()"
      ],
      "metadata": {
        "id": "5QOdnDSV2Bqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.concat([train_df, df_multi], ignore_index=True, sort=False)"
      ],
      "metadata": {
        "id": "dg_kUnbct9jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_bkp = train_df.copy()"
      ],
      "metadata": {
        "id": "NI59kKZmuCez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c9e30f-ccb1-4833-de78-016714e1d16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1367\n",
              "1    1274\n",
              "2    1118\n",
              "Name: Spoiler_Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df_merged.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDaZvLBAAgf8",
        "outputId": "ff7bba1f-523b-46a1-b27b-5b98bcf4e5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"concat_input\"] = train_df[['Post','Content','Title']].apply(lambda x: x['Post'] + ' ? ' + x['Title'] + ' -- ' + x['Content'], axis=1)\n",
        "test_df[\"concat_input\"] = test_df[['Post','Content','Title']].apply(lambda x: x['Post'] + ' ? ' + x['Title'] + ' -- ' + x['Content'], axis=1)"
      ],
      "metadata": {
        "id": "85SSZAj5uO_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop([\"Post\"], inplace = True, axis = 1)\n",
        "train_df.drop([\"Content\"], inplace = True, axis = 1)\n",
        "train_df.drop([\"Title\"], inplace = True, axis = 1)\n",
        "test_df.drop([\"Post\"], inplace = True, axis = 1)\n",
        "test_df.drop([\"Content\"], inplace = True, axis = 1)\n",
        "test_df.drop([\"Title\"], inplace = True, axis = 1)"
      ],
      "metadata": {
        "id": "HvfoBW4M2MiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Spoiler_Type'].value_counts()"
      ],
      "metadata": {
        "id": "z0NHd2ll2Uhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() "
      ],
      "metadata": {
        "id": "CUFg2_jc2T_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66cfa56-f87d-4eed-c479-1343dc5450eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "# Load pre-trained RoBERTa model and tokenizer\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "18ad715675774f56a3cf89e4c5d2ddab",
            "9f474458377f427e9c57549094b73d41",
            "8f7c6e45ecc44ff5afa0304160177e0d",
            "3d0dd7d2959c40849476b531684e2503",
            "3423c023731441d4a24f974502b5ceee",
            "43a88341c7fc4e828ab24e36786dabe9",
            "73a29adb8dc646c7aef07ad87402f212",
            "7dd0e9e411324139a333cda9043520d2",
            "7eb100008a86413394f4e007ac120f7f",
            "48ec79183aaa4f7f9c59f5b68e65cb3c",
            "66b7d643ddd24fbd97c1da7132c2fc8d"
          ]
        },
        "id": "zsaZ7g5ZP3Kf",
        "outputId": "035f2f1b-3f22-4be8-a02c-bab6d01e146e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18ad715675774f56a3cf89e4c5d2ddab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiJherhPnxSq",
        "outputId": "dabc44ac-01fa-4dec-e9b0-fe7a6d364d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train = list(train_df[\"concat_input\"])\n",
        "y_train = list(train_df[\"Spoiler_Type\"])\n",
        "X_test = list(test_df[\"concat_input\"])\n",
        "y_test = list(test_df[\"Spoiler_Type\"])\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1,stratify=y_train)\n",
        "# X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size = 0.5, stratify=y_val)"
      ],
      "metadata": {
        "id": "Ev3YLwudP6b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "# X_val = np.array(X_val)\n",
        "y_train = np.array(y_train)\n",
        "# y_val = np.array(y_val)\n",
        "\n",
        "# Prepare training and validation data\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=512)\n",
        "# val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=512)\n",
        "train_labels = torch.tensor(y_train)\n",
        "# val_labels = torch.tensor(y_val)"
      ],
      "metadata": {
        "id": "qBKibeYwP6Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
        "train_attention_mask = torch.tensor(train_encodings['attention_mask'])\n",
        "train_labels = torch.tensor(y_train).to(device)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_input_ids.to(device), train_attention_mask.to(device), train_labels)"
      ],
      "metadata": {
        "id": "PxM_GuZDP5ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer and learning rate scheduler\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=4e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "# Set up training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    # Train model for one epoch\n",
        "    model.train()\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # # Evaluate model on validation set\n",
        "    # model.eval()\n",
        "    # val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "    # with torch.no_grad():\n",
        "    #     num_correct = 0\n",
        "    #     num_total = 0\n",
        "    #     for batch in val_loader:\n",
        "    #         inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
        "    #         outputs = model(**inputs)\n",
        "    #         logits = outputs.logits\n",
        "    #         preds = torch.argmax(logits, dim=1)\n",
        "    #         num_correct += torch.sum(preds == batch[2])\n",
        "    #         num_total += len(batch[2])\n",
        "    #     acc = num_correct / num_total\n",
        "    #     print(f'Epoch {epoch+1} - val accuracy: {acc:.4f}')\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "BZGe8KGXu6pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Prepare training and validation data\n",
        "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512)\n",
        "test_labels = torch.tensor(y_test)\n",
        "\n",
        "test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
        "test_attention_mask = torch.tensor(test_encodings['attention_mask'])\n",
        "test_labels = torch.tensor(y_test).to(device)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_input_ids.to(device), test_attention_mask.to(device), test_labels)"
      ],
      "metadata": {
        "id": "R5jVXs246BD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = '/content/drive/MyDrive/NLP_dataset/models/roberta_classification_model.pt'"
      ],
      "metadata": {
        "id": "qSznmpF0VdAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the evaluation dataset and make predictions\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model.eval()\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    for batch in test_loader:\n",
        "        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        y_true.extend(batch[2].tolist())\n",
        "        y_pred.extend(preds.tolist())\n",
        "        num_correct += torch.sum(preds == batch[2])\n",
        "        num_total += len(batch[2])\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "print(f'F1 score: {f1:.4f}')\n",
        "#Prev acc\n",
        "acc = num_correct / num_total\n",
        "print(f'Test accuracy: {acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h43v8GpkWEYK",
        "outputId": "ee4df3f6-b0b0-4bdb-aa8c-c3b3ec0a52a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6963\n",
            "F1 score: 0.6959\n",
            "Test accuracy: 0.6962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUBoe_BD8pJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2896c03-4b9b-42ae-9b54-fc3c45f4d322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6963\n",
            "F1 score: 0.6959\n",
            "Test accuracy: 0.6962\n"
          ]
        }
      ],
      "source": [
        "# Iterate over the evaluation dataset and make predictions\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model.eval()\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    for batch in test_loader:\n",
        "        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        y_true.extend(batch[2].tolist())\n",
        "        y_pred.extend(preds.tolist())\n",
        "        num_correct += torch.sum(preds == batch[2])\n",
        "        num_total += len(batch[2])\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "print(f'F1 score: {f1:.4f}')\n",
        "#Prev acc\n",
        "acc = num_correct / num_total\n",
        "print(f'Test accuracy: {acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phrase"
      ],
      "metadata": {
        "id": "DWcrmoadtiSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset, load_metric\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from datasets.load import DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import gc\n",
        "import collections\n",
        "import evaluate\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import default_data_collator\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "SSNWkwsUTOlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Palak/microsoft_deberta-large_squad\"\n",
        "batch_size = 2"
      ],
      "metadata": {
        "id": "EHtg07xFofAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetFormatter:\n",
        "  def __init__(self) -> None:\n",
        "    self.train_df = pd.read_json(\"train.jsonl\", lines = True)\n",
        "    self.test_df = pd.read_json(\"validation.jsonl\", lines = True)\n",
        "\n",
        "  def list_to_string(self, spoiler_type):\n",
        "    if spoiler_type[0] == \"phrase\":\n",
        "      return 0\n",
        "    elif spoiler_type[0] == \"passage\":\n",
        "      return 1\n",
        "    elif spoiler_type[0] == \"multi\":\n",
        "      return 2\n",
        "\n",
        "  def return_text(self, text):\n",
        "    return text[0]\n",
        "\n",
        "  def preprocess_data(self, data):\n",
        "\n",
        "    no_of_rows = data.shape[0]\n",
        "    formatted_data = []\n",
        "    for index in range(no_of_rows):\n",
        "      complete_description = \" \".join(data.iloc[index].to_dict()[\"targetParagraphs\"])\n",
        "      row = {}\n",
        "      row[\"id\"] = data.iloc[index].to_dict()[\"uuid\"]\n",
        "      row[\"context\"] = complete_description,\n",
        "      row[\"question\"] = data.iloc[index].to_dict()[\"postText\"][0],\n",
        "      row[\"answers\"] = {\n",
        "          \"text\": data.iloc[index].to_dict()[\"spoiler\"],\n",
        "          \"answer_start\": [complete_description.find(data.iloc[index].to_dict()[\"spoiler\"][0])]\n",
        "      }\n",
        "      formatted_data.append(row)\n",
        "    \n",
        "    return formatted_data\n",
        "\n",
        "  def get_formatted_dataset(self):\n",
        "    train_df = self.train_df\n",
        "    test_df = self.test_df\n",
        "\n",
        "    train_df[\"tags\"] = train_df[\"tags\"].apply(self.list_to_string)\n",
        "    test_df[\"tags\"] = test_df[\"tags\"].apply(self.list_to_string)\n",
        "\n",
        "    train_df = train_df[train_df['tags']==0]\n",
        "    test_df = test_df[test_df['tags']==0]\n",
        "\n",
        "    train_df = dataset_formatte_obj.preprocess_data(train_df)\n",
        "    test_df = dataset_formatte_obj.preprocess_data(test_df)\n",
        "\n",
        "    train_df= pd.DataFrame(train_df)\n",
        "    test_df = pd.DataFrame(test_df)\n",
        "\n",
        "    train_df[\"context\"] = train_df[\"context\"].apply(self.return_text)\n",
        "    test_df[\"context\"] = test_df[\"context\"].apply(self.return_text)\n",
        "\n",
        "\n",
        "    train_df[\"question\"] = train_df[\"question\"].apply(self.return_text)\n",
        "    test_df[\"question\"] = test_df[\"question\"].apply(self.return_text)\n",
        "\n",
        "    validation_df = train_df.iloc[1258:]\n",
        "    train_df = train_df.iloc[:1174]\n",
        "\n",
        "    dataset_train = Dataset.from_pandas(train_df)\n",
        "    dataset_validation = Dataset.from_pandas(validation_df)\n",
        "    dataset_test =  Dataset.from_pandas(test_df)\n",
        "\n",
        "    datasets = DatasetDict()\n",
        "\n",
        "    datasets[\"train\"] = dataset_train\n",
        "    datasets[\"validation\"] = dataset_validation\n",
        "    datasets[\"test\"] = dataset_test\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "dataset_formatte_obj = DatasetFormatter()\n",
        "\n",
        "datasets = dataset_formatte_obj.get_formatted_dataset()"
      ],
      "metadata": {
        "id": "V3Dcn4raoiqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor:\n",
        "  def __init__(self, model_checkpoint, max_length, doc_stride) -> None:\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    self.max_length = max_length\n",
        "    self.doc_stride = doc_stride\n",
        "    self.pad_on_right = self.tokenizer.padding_side == \"right\"\n",
        "\n",
        "  def prepare_train_features(self, examples):\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    tokenized_examples = self.tokenizer(\n",
        "        examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "        examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "        max_length=self.max_length,\n",
        "        stride=self.doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions, end_positions = [], []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if self.pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if self.pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                start_positions.append(cls_index)\n",
        "                end_positions.append(cls_index)\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                start_positions.append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                end_positions.append(token_end_index + 1)\n",
        "    tokenized_examples[\"start_positions\"] = start_positions\n",
        "    tokenized_examples[\"end_positions\"] = end_positions\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "max_length = 384\n",
        "doc_stride = 128\n",
        "proprocessor_obj = Preprocessor(model_checkpoint, max_length, doc_stride)\n",
        "tokenized_datasets = datasets.map(proprocessor_obj.prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "_oVDQM_wookC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTune:\n",
        "  def __init__(self, model_checkpoint) -> None:\n",
        "    self.model_checkpoint = model_checkpoint\n",
        "    self.model = AutoModelForQuestionAnswering.from_pretrained(self.model_checkpoint)\n",
        "    self.setup()\n",
        "    self.trainer = None\n",
        "\n",
        "  def setup(self):\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    self.model.to(device)\n",
        "    print(f'Working on {device}')\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU Cache removed\")\n",
        "\n",
        "  def trainer_model(self):\n",
        "    model_name = self.model_checkpoint.split(\"/\")[-1]\n",
        "    args = TrainingArguments(\n",
        "        f\"{model_name}-finetuned-webis\",\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=1,\n",
        "        weight_decay=0.01,\n",
        "        push_to_hub=False,\n",
        "    )\n",
        "\n",
        "    data_collator = default_data_collator\n",
        "    trainer = Trainer(\n",
        "    self.model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=proprocessor_obj.tokenizer,\n",
        "    )\n",
        "    self.trainer = trainer\n",
        "\n",
        "  def get_trainer(self):\n",
        "    return self.trainer\n"
      ],
      "metadata": {
        "id": "ZcF02nGFourW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "MNTcJIHdrYxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "id": "ySMEh0BHqtRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_obj = FineTune(model_checkpoint)\n",
        "fine_tune_obj.trainer_model()\n",
        "trainer = fine_tune_obj.get_trainer()"
      ],
      "metadata": {
        "id": "rGkllxsHozjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "QKv100GVo4Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Z-ZVeC2so8Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datasets = DatasetDict()\n",
        "test_datasets[\"test\"] = datasets[\"test\"]"
      ],
      "metadata": {
        "id": "y9Hvr-xYpBCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ],
      "metadata": {
        "id": "wpLCkyarpBsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluate:\n",
        "  def __init__(self, proprocessor_obj):\n",
        "    self.tokenizer = proprocessor_obj.tokenizer\n",
        "    # The maximum length of a feature (question and context)\n",
        "    self.max_length = proprocessor_obj.max_length\n",
        "    self.doc_stride = proprocessor_obj.doc_stride\n",
        "    self.pad_on_right = self.tokenizer.padding_side == \"right\"\n",
        "\n",
        "\n",
        "  def prepare_validation_features(self, examples):\n",
        "      examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "      tokenized_examples = proprocessor_obj.tokenizer(\n",
        "          examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "          examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "          truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "          max_length=max_length,\n",
        "          stride=doc_stride,\n",
        "          return_overflowing_tokens=True,\n",
        "          return_offsets_mapping=True,\n",
        "          padding=\"max_length\",\n",
        "      )\n",
        "      sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "      tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "      for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "          sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "          context_index = 1 if self.pad_on_right else 0\n",
        "\n",
        "          sample_index = sample_mapping[i]\n",
        "          tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "          tokenized_examples[\"offset_mapping\"][i] = [\n",
        "              (o if sequence_ids[k] == context_index else None)\n",
        "              for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "          ]\n",
        "\n",
        "      return tokenized_examples\n",
        "  \n",
        "  def prepare_test_features(self, examples):\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    tokenized_examples = self.tokenizer(\n",
        "        examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "        examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if self.pad_on_right else 0\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "  def postprocess_qa_predictions(self, examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)  \n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None \n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(self.tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "evaluate_obj = Evaluate(proprocessor_obj)"
      ],
      "metadata": {
        "id": "3jWuDRwSpEi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    evaluate_obj.prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")\n",
        "raw_predictions = trainer.predict(validation_features)\n",
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ],
      "metadata": {
        "id": "NlaCaN0qpOSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = datasets[\"test\"].map(\n",
        "    evaluate_obj.prepare_test_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"test\"].column_names\n",
        ")\n",
        "raw_predictions = trainer.predict(test_features)\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))"
      ],
      "metadata": {
        "id": "8vgfleezpYWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = evaluate_obj.postprocess_qa_predictions(test_datasets[\"test\"], test_features, raw_predictions.predictions)"
      ],
      "metadata": {
        "id": "U_JIjzLpphjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"squad\")"
      ],
      "metadata": {
        "id": "zEklcf2UpkMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_datasets[\"test\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ],
      "metadata": {
        "id": "F-cknH7SqDp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluationMetric:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.bleu = evaluate.load('bleu')\n",
        "    self.meteor = evaluate.load('meteor')\n",
        "    self.bertscore = load(\"bertscore\")\n",
        "\n",
        "  def get_bleu_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    count = 0\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          results += self.bleu.compute(predictions = [f'\"{each[\"prediction_text\"].strip()}\"'], references = [[f'\"{ref[\"answers\"][\"text\"][0].strip()}\"']]).get(\"bleu\")\n",
        "\n",
        "    bleu_score = results/count\n",
        "    return bleu_score\n",
        "\n",
        "  def get_meteor_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    count = 0\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          results += self.meteor.compute(predictions = [f'\"{each[\"prediction_text\"].strip()}\"'], references = [f'\"{ref[\"answers\"][\"text\"][0].strip()}\"']).get(\"meteor\")\n",
        "\n",
        "    meteor_score = results/count\n",
        "    return meteor_score\n",
        "  \n",
        "  def get_bert_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          results = self.bertscore.compute(predictions=[f'\"{each[\"prediction_text\"].strip()}\"'], references=[f'\"{ref[\"answers\"][\"text\"][0].strip()}\"'], lang=\"en\")\n",
        "          total_precision += results.get(\"precision\")[0]\n",
        "          total_recall += results.get(\"recall\")[0]\n",
        "          total_f1 += results.get(\"f1\")[0]\n",
        "\n",
        "    avg_precision = total_precision/count\n",
        "    avg_recall = total_recall/count\n",
        "    avg_f1 = total_f1/count\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1\n",
        "    "
      ],
      "metadata": {
        "id": "h3vDrDZrqEKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_obj = EvaluationMetric()\n",
        "bleu_score = evaluation_metric_obj.get_bleu_score(formatted_predictions, references)\n",
        "meteor_score = evaluation_metric_obj.get_meteor_score(formatted_predictions, references)\n",
        "avg_precision, avg_recall, avg_f1 = evaluation_metric_obj.get_bert_score(formatted_predictions, references)\n",
        "\n",
        "print(\"BLEU Score: \", bleu_score)\n",
        "print(\"METEOR Score: \", meteor_score)\n",
        "print(\"Avg. Precision value: \", avg_precision)\n",
        "print(\"Avg. Recall value: \", avg_recall)\n",
        "print(\"Avg. F1 value: \", avg_f1)"
      ],
      "metadata": {
        "id": "ohke95JxqI7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passage"
      ],
      "metadata": {
        "id": "iPo4mGbfTWl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"thatdramebaazguy/roberta-base-squad\"\n",
        "batch_size = 2"
      ],
      "metadata": {
        "id": "OydPZow0TSpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetFormatter:\n",
        "  def __init__(self) -> None:\n",
        "    self.train_df = pd.read_json(\"train.jsonl\", lines = True)\n",
        "    self.test_df = pd.read_json(\"validation.jsonl\", lines = True)\n",
        "\n",
        "  def list_to_string(self, spoiler_type):\n",
        "    if spoiler_type[0] == \"phrase\":\n",
        "      return 0\n",
        "    elif spoiler_type[0] == \"passage\":\n",
        "      return 1\n",
        "    elif spoiler_type[0] == \"multi\":\n",
        "      return 2\n",
        "\n",
        "  def return_text(self, text):\n",
        "    return text[0]\n",
        "\n",
        "  def preprocess_data(self, data):\n",
        "\n",
        "    no_of_rows = data.shape[0]\n",
        "    formatted_data = []\n",
        "    for index in range(no_of_rows):\n",
        "      complete_description = \" \".join(data.iloc[index].to_dict()[\"targetParagraphs\"])\n",
        "      row = {}\n",
        "      row[\"id\"] = data.iloc[index].to_dict()[\"uuid\"]\n",
        "      row[\"context\"] = complete_description,\n",
        "      row[\"question\"] = data.iloc[index].to_dict()[\"postText\"][0],\n",
        "      row[\"answers\"] = {\n",
        "          \"text\": data.iloc[index].to_dict()[\"spoiler\"],\n",
        "          \"answer_start\": [complete_description.find(data.iloc[index].to_dict()[\"spoiler\"][0])]\n",
        "      }\n",
        "      formatted_data.append(row)\n",
        "    \n",
        "    return formatted_data\n",
        "\n",
        "  def get_formatted_dataset(self):\n",
        "    train_df = self.train_df\n",
        "    test_df = self.test_df\n",
        "\n",
        "    train_df[\"tags\"] = train_df[\"tags\"].apply(self.list_to_string)\n",
        "    test_df[\"tags\"] = test_df[\"tags\"].apply(self.list_to_string)\n",
        "\n",
        "    # Taking only Pharse dataset\n",
        "    train_df = train_df[train_df['tags']==1]\n",
        "    test_df = test_df[test_df['tags']==1]\n",
        "\n",
        "    train_df = dataset_formatte_obj.preprocess_data(train_df)\n",
        "    test_df = dataset_formatte_obj.preprocess_data(test_df)\n",
        "\n",
        "    train_df= pd.DataFrame(train_df)\n",
        "    test_df = pd.DataFrame(test_df)\n",
        "\n",
        "    # Removing the list\n",
        "    train_df[\"context\"] = train_df[\"context\"].apply(self.return_text)\n",
        "    test_df[\"context\"] = test_df[\"context\"].apply(self.return_text)\n",
        "\n",
        "    # Removing the list\n",
        "    train_df[\"question\"] = train_df[\"question\"].apply(self.return_text)\n",
        "    test_df[\"question\"] = test_df[\"question\"].apply(self.return_text)\n",
        "\n",
        "    # Spliting the train and validation set from training dataset\n",
        "    validation_df = train_df.iloc[1258:]\n",
        "    train_df = train_df.iloc[:1174]\n",
        "\n",
        "    dataset_train = Dataset.from_pandas(train_df)\n",
        "    dataset_validation = Dataset.from_pandas(validation_df)\n",
        "    dataset_test =  Dataset.from_pandas(test_df)\n",
        "\n",
        "    datasets = DatasetDict()\n",
        "\n",
        "    datasets[\"train\"] = dataset_train\n",
        "    datasets[\"validation\"] = dataset_validation\n",
        "    datasets[\"test\"] = dataset_test\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "dataset_formatte_obj = DatasetFormatter()\n",
        "\n",
        "datasets = dataset_formatte_obj.get_formatted_dataset()"
      ],
      "metadata": {
        "id": "yF8X1qksqZXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor:\n",
        "  def __init__(self, model_checkpoint, max_length, doc_stride) -> None:\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    self.max_length = max_length\n",
        "    self.doc_stride = doc_stride\n",
        "    self.pad_on_right = self.tokenizer.padding_side == \"right\"\n",
        "\n",
        "  def prepare_train_features(self, examples):\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    tokenized_examples = self.tokenizer(\n",
        "        examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "        examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "        max_length=self.max_length,\n",
        "        stride=self.doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "\n",
        "    start_positions, end_positions = [], []\n",
        "\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
        "\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if self.pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if self.pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                start_positions.append(cls_index)\n",
        "                end_positions.append(cls_index)\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                start_positions.append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                end_positions.append(token_end_index + 1)\n",
        "\n",
        "    tokenized_examples[\"start_positions\"] = start_positions\n",
        "    tokenized_examples[\"end_positions\"] = end_positions\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "max_length = 384\n",
        "doc_stride = 128\n",
        "proprocessor_obj = Preprocessor(model_checkpoint, max_length, doc_stride)\n",
        "tokenized_datasets = datasets.map(proprocessor_obj.prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "JxFuhyjzqegm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTune:\n",
        "  def __init__(self, model_checkpoint) -> None:\n",
        "    self.model_checkpoint = model_checkpoint\n",
        "    self.model = AutoModelForQuestionAnswering.from_pretrained(self.model_checkpoint)\n",
        "    self.setup()\n",
        "    self.trainer = None\n",
        "\n",
        "  def setup(self):\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    self.model.to(device)\n",
        "    print(f'Working on {device}')\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU Cache removed\")\n",
        "\n",
        "  def trainer_model(self):\n",
        "    model_name = self.model_checkpoint.split(\"/\")[-1]\n",
        "    args = TrainingArguments(\n",
        "        f\"{model_name}-finetuned-webis\",\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=1,\n",
        "        weight_decay=0.01,\n",
        "        push_to_hub=False,\n",
        "    )\n",
        "\n",
        "    data_collator = default_data_collator\n",
        "    trainer = Trainer(\n",
        "    self.model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=proprocessor_obj.tokenizer,\n",
        "    )\n",
        "    self.trainer = trainer\n",
        "\n",
        "  def get_trainer(self):\n",
        "    return self.trainer\n"
      ],
      "metadata": {
        "id": "UsZm7OpYqkEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_obj = FineTune(model_checkpoint)\n",
        "fine_tune_obj.trainer_model()\n",
        "trainer = fine_tune_obj.get_trainer()"
      ],
      "metadata": {
        "id": "0-rHr70aqqVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Z4FRUNseqsvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "89r6d0l5qvKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datasets = DatasetDict()\n",
        "test_datasets[\"test\"] = datasets[\"test\"]"
      ],
      "metadata": {
        "id": "KM5w6lRYqy1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ],
      "metadata": {
        "id": "OqkbTrEgq5S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Evaluate:\n",
        "  def __init__(self, proprocessor_obj):\n",
        "    self.tokenizer = proprocessor_obj.tokenizer\n",
        "    self.max_length = proprocessor_obj.max_length\n",
        "    self.doc_stride = proprocessor_obj.doc_stride\n",
        "    self.pad_on_right = self.tokenizer.padding_side == \"right\"\n",
        "\n",
        "\n",
        "  def prepare_validation_features(self, examples):\n",
        "      examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "      tokenized_examples = proprocessor_obj.tokenizer(\n",
        "          examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "          examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "          truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "          max_length=max_length,\n",
        "          stride=doc_stride,\n",
        "          return_overflowing_tokens=True,\n",
        "          return_offsets_mapping=True,\n",
        "          padding=\"max_length\",\n",
        "      )\n",
        "\n",
        "      sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "      tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "      for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "          sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "          context_index = 1 if self.pad_on_right else 0\n",
        "\n",
        "\n",
        "          sample_index = sample_mapping[i]\n",
        "          tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "          tokenized_examples[\"offset_mapping\"][i] = [\n",
        "              (o if sequence_ids[k] == context_index else None)\n",
        "              for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "          ]\n",
        "\n",
        "      return tokenized_examples\n",
        "  \n",
        "  def prepare_test_features(self, examples):\n",
        "\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    tokenized_examples = self.tokenizer(\n",
        "        examples[\"question\" if self.pad_on_right else \"context\"],\n",
        "        examples[\"context\" if self.pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if self.pad_on_right else 0\n",
        "\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "  def postprocess_qa_predictions(self, examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)  \n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None \n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(self.tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "evaluate_obj = Evaluate(proprocessor_obj)"
      ],
      "metadata": {
        "id": "za03oBdqq7yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    evaluate_obj.prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")\n",
        "raw_predictions = trainer.predict(validation_features)\n",
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ],
      "metadata": {
        "id": "YupHIpltrDTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = datasets[\"test\"].map(\n",
        "    evaluate_obj.prepare_test_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"test\"].column_names\n",
        ")\n",
        "raw_predictions = trainer.predict(test_features)\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))"
      ],
      "metadata": {
        "id": "fwlNqfFIrI43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = evaluate_obj.postprocess_qa_predictions(test_datasets[\"test\"], test_features, raw_predictions.predictions)"
      ],
      "metadata": {
        "id": "lX8ErNXSrLPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"squad\")"
      ],
      "metadata": {
        "id": "MHLAy2mUrOfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_datasets[\"test\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ],
      "metadata": {
        "id": "ahV-4w7ArRwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluationMetric:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.bleu = evaluate.load('bleu')\n",
        "    self.meteor = evaluate.load('meteor')\n",
        "    self.bertscore = load(\"bertscore\")\n",
        "\n",
        "  def list_to_string(self, spoiler_type):\n",
        "    if spoiler_type[0] == \"phrase\":\n",
        "      return 0\n",
        "    elif spoiler_type[0] == \"passage\":\n",
        "      return 1\n",
        "    elif spoiler_type[0] == \"multi\":\n",
        "      return 2\n",
        "\n",
        "  def test_target_paragraphs(self, data):\n",
        "    target_paragraphs = []\n",
        "    for row in data.values.tolist():\n",
        "      target_paragraphs.append((row[0],row[4]))\n",
        "    return target_paragraphs\n",
        "  \n",
        "  def return_target_paragraph(self, id, prediction_text):\n",
        "    test_df = pd.read_json(\"validation.jsonl\", lines = True)\n",
        "    test_df[\"tags\"] = test_df[\"tags\"].apply(self.list_to_string)\n",
        "    test_df = test_df[test_df[\"tags\"] == 1]\n",
        "    target_paragraphs = self.test_target_paragraphs(test_df)\n",
        "\n",
        "    for each in target_paragraphs:\n",
        "      if each[0] == id:\n",
        "        for sentence in each[1]:\n",
        "          if prediction_text in sentence:\n",
        "            return sentence\n",
        "\n",
        "    return prediction_text \n",
        "\n",
        "  def get_bleu_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    count = 0\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          prediction = self.return_target_paragraph(each[\"id\"],each[\"prediction_text\"])\n",
        "          results += self.bleu.compute(predictions = [f'\"{prediction.strip()}\"'], references = [[f'\"{ref[\"answers\"][\"text\"][0].strip()}\"']]).get(\"bleu\")\n",
        "\n",
        "    bleu_score = results/count\n",
        "    return bleu_score\n",
        "\n",
        "  def get_meteor_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    count = 0\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          prediction = self.return_target_paragraph(each[\"id\"],each[\"prediction_text\"])\n",
        "          results += self.meteor.compute(predictions = [f'\"{prediction.strip()}\"'], references = [f'\"{ref[\"answers\"][\"text\"][0].strip()}\"']).get(\"meteor\")\n",
        "\n",
        "    meteor_score = results/count\n",
        "    return meteor_score\n",
        "  \n",
        "  def get_bert_score(self, formatted_predictions, references):\n",
        "    results = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for each in formatted_predictions:\n",
        "      count += 1\n",
        "      for ref in references:\n",
        "\n",
        "        if each[\"id\"] == ref[\"id\"]:\n",
        "          prediction = self.return_target_paragraph(each[\"id\"],each[\"prediction_text\"])\n",
        "          results = self.bertscore.compute(predictions=[f'\"{prediction.strip()}\"'], references=[f'\"{ref[\"answers\"][\"text\"][0].strip()}\"'], lang=\"en\")\n",
        "          total_precision += results.get(\"precision\")[0]\n",
        "          total_recall += results.get(\"recall\")[0]\n",
        "          total_f1 += results.get(\"f1\")[0]\n",
        "\n",
        "    avg_precision = total_precision/count\n",
        "    avg_recall = total_recall/count\n",
        "    avg_f1 = total_f1/count\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1   "
      ],
      "metadata": {
        "id": "WKMmOt3YrUD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_obj = EvaluationMetric()\n",
        "bleu_score = evaluation_metric_obj.get_bleu_score(formatted_predictions, references)\n",
        "meteor_score = evaluation_metric_obj.get_meteor_score(formatted_predictions, references)\n",
        "avg_precision, avg_recall, avg_f1 = evaluation_metric_obj.get_bert_score(formatted_predictions, references)\n",
        "\n",
        "print(\"BLEU Score: \", bleu_score)\n",
        "print(\"METEOR Score: \", meteor_score)\n",
        "print(\"Avg. Precision value: \", avg_precision)\n",
        "print(\"Avg. Recall value: \", avg_recall)\n",
        "print(\"Avg. F1 value: \", avg_f1)"
      ],
      "metadata": {
        "id": "fKA7hSK5rYcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "g_CepvWhbUYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install datasets\n",
        "# !pip3 install pytorch-lightning==1.2.7\n",
        "# !pip3 install torchtext==0.6 torch"
      ],
      "metadata": {
        "id": "597rK-E7bfdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast as T5Tokenizer\n",
        ")\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "from torch.utils.data import  Dataset,DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style=\"whitegrid\", palette='muted', font_scale = 1.2)\n",
        "rcParams['figure.figsize'] = 16,10"
      ],
      "metadata": {
        "id": "S1EsAt6nb30z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"t5-base\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "-JwpCOqZb7Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_json(\"train.jsonl\", lines = True)\n",
        "test_df = pd.read_json(\"validation.jsonl\", lines = True)"
      ],
      "metadata": {
        "id": "Us2ap3B9b9uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_string(spoiler_type):\n",
        "  if spoiler_type[0] == \"phrase\":\n",
        "    return 0\n",
        "  elif spoiler_type[0] == \"passage\":\n",
        "    return 1\n",
        "  elif spoiler_type[0] == \"multi\":\n",
        "    return 2"
      ],
      "metadata": {
        "id": "OHvRKxnUhOjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"tags\"] = train_df[\"tags\"].apply(list_to_string)\n",
        "test_df[\"tags\"] = test_df[\"tags\"].apply(list_to_string)"
      ],
      "metadata": {
        "id": "cILjCEFLhSYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[train_df['tags']==2]\n",
        "test_df = test_df[test_df['tags']==2]"
      ],
      "metadata": {
        "id": "H7ixMFg6hTM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "\n",
        "  no_of_rows = data.shape[0]\n",
        "  formatted_data = []\n",
        "  for index in range(no_of_rows):\n",
        "    complete_description = \" \".join(data.iloc[index].to_dict()[\"targetParagraphs\"])\n",
        "    row = {}\n",
        "    row[\"id\"] = data.iloc[index].to_dict()[\"uuid\"]\n",
        "    row[\"context\"] = complete_description,\n",
        "    row[\"question\"] = data.iloc[index].to_dict()[\"postText\"][0],\n",
        "    row[\"answers\"] = {\n",
        "        \"text\": data.iloc[index].to_dict()[\"spoiler\"],\n",
        "        \"answer_start\": [complete_description.find(data.iloc[index].to_dict()[\"spoiler\"][0])]\n",
        "    }\n",
        "    formatted_data.append(row)\n",
        "  \n",
        "  return formatted_data"
      ],
      "metadata": {
        "id": "NV0XK6rLhVAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = preprocess_data(train_df)\n",
        "test_df = preprocess_data(test_df)"
      ],
      "metadata": {
        "id": "Jdg62X-Wheof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df= pd.DataFrame(train_df)\n",
        "test_df = pd.DataFrame(test_df)"
      ],
      "metadata": {
        "id": "IfaGRt6YhiZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_text(text):\n",
        "  return text[0]"
      ],
      "metadata": {
        "id": "BjvXsbAfhjDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"context\"] = train_df[\"context\"].apply(return_text)\n",
        "test_df[\"context\"] = test_df[\"context\"].apply(return_text)"
      ],
      "metadata": {
        "id": "EYQxEgkXhlXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"question\"] = train_df[\"question\"].apply(return_text)\n",
        "test_df[\"question\"] = test_df[\"question\"].apply(return_text)"
      ],
      "metadata": {
        "id": "rSNsxW61ho5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "N-CwhZDsht2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "VAAlujoNhuUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_answers(text):\n",
        "  return \",\".join(text[\"text\"])"
      ],
      "metadata": {
        "id": "F3vhIOOHhv6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"answers\"] = train_df[\"answers\"].apply(preprocess_answers)"
      ],
      "metadata": {
        "id": "NLBKPV4nhyYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"answers\"] = test_df[\"answers\"].apply(preprocess_answers)"
      ],
      "metadata": {
        "id": "lwOr6Rymh0WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[[\"context\", \"question\", \"answers\"]]\n",
        "test_df = test_df[[\"context\", \"question\", \"answers\"]]"
      ],
      "metadata": {
        "id": "upY8l0hxh2cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def join_context_question(data):\n",
        "  final_df = []\n",
        "  for each in data:\n",
        "    final_df.append([each[0]+\" ? \"+each[1], each[2]])\n",
        "  \n",
        "  return pd.DataFrame(final_df, columns = [\"context\", \"answers\"])"
      ],
      "metadata": {
        "id": "BEiQQnIjh5ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = join_context_question(train_df.values.tolist())"
      ],
      "metadata": {
        "id": "bYVZQ8L9iCEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = join_context_question(test_df.values.tolist())"
      ],
      "metadata": {
        "id": "gio2DvDviDyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"context\"] = train_df[\"context\"].str.encode(\"ascii\", \"ignore\").str.decode(\"ascii\")\n",
        "train_df[\"answers\"] = train_df[\"answers\"].str.encode(\"ascii\", \"ignore\").str.decode(\"ascii\")"
      ],
      "metadata": {
        "id": "z_JGUatTiGX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"context\"] = test_df[\"context\"].str.encode(\"ascii\", \"ignore\").str.decode(\"ascii\")\n",
        "test_df[\"answers\"] = test_df[\"answers\"].str.encode(\"ascii\", \"ignore\").str.decode(\"ascii\")"
      ],
      "metadata": {
        "id": "BKqOYkHYiHUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClickbaitSummaryDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               data,\n",
        "               tokenizer,\n",
        "               text_max_token_len= 512,\n",
        "               summary_max_token_len= 128):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = data\n",
        "    self.text_max_token_len = text_max_token_len\n",
        "    self.summary_max_token_len = summary_max_token_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    data_row = self.data.iloc[index]\n",
        "\n",
        "    text = data_row[\"context\"]\n",
        "\n",
        "    text_encoding = self.tokenizer(text,\n",
        "                              max_length= self.text_max_token_len,\n",
        "                              padding = \"max_length\",\n",
        "                              truncation=True,\n",
        "                              return_attention_mask = True,\n",
        "                              add_special_tokens = True,\n",
        "                              return_tensors = \"pt\"\n",
        "                              )\n",
        "    summary_encoding = self.tokenizer(data_row[\"answers\"],\n",
        "                              max_length= self.text_max_token_len,\n",
        "                              padding = \"max_length\",\n",
        "                              truncation=True,\n",
        "                              return_attention_mask = True,\n",
        "                              add_special_tokens = True,\n",
        "                              return_tensors = \"pt\"\n",
        "                              )\n",
        "    labels = summary_encoding[\"input_ids\"]\n",
        "    labels[labels==0] = -100\n",
        "\n",
        "    return dict(text=text, summary = data_row[\"answers\"], text_input_ids=text_encoding[\"input_ids\"].flatten(),\n",
        "                                              text_attention_mask = text_encoding[\"attention_mask\"].flatten(),\n",
        "                                              labels = labels.flatten(),\n",
        "                                              labels_attention_mask=summary_encoding[\"attention_mask\"].flatten()\n",
        "                                              )"
      ],
      "metadata": {
        "id": "2SDU-jckiJZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClickbaitSummaryDataModule(pl.LightningDataModule):\n",
        "  def __int__(self, train_df, test_df, tokenizer, BATCH_SIZE):\n",
        "    super().__init__()\n",
        "  \n",
        "  def setup(self, stage=None):\n",
        "    self.train_dataset = ClickbaitSummaryDataset(\n",
        "        self.train_df,\n",
        "        self.tokenizer,\n",
        "        self.text_max_token_len,\n",
        "        self.summary_max_token_len\n",
        "    )\n",
        "    self.test_dataset = ClickbaitSummaryDataset(\n",
        "        self.test_df,\n",
        "        self.tokenizer,\n",
        "        self.text_max_token_len,\n",
        "        self.summary_max_token_len\n",
        "    )\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle=True, num_workers = 2)\n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle = True, num_workers = 2)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle = True, num_workers = 2)"
      ],
      "metadata": {
        "id": "sMsn6hcfiJUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClickbaitSummaryModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, decoder_attention_mask, labels = None):\n",
        "    output = self.model(\n",
        "        input_ids,\n",
        "        attention_mask= attention_mask,\n",
        "        labels = labels,\n",
        "        decoder_attention_mask = decoder_attention_mask\n",
        "    )\n",
        "\n",
        "    return output.loss, output.logits\n",
        "\n",
        "  def training_step(self, batch, batch_size):\n",
        "    input_ids = batch[\"text_input_ids\"]\n",
        "    attention_mask = batch[\"text_attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "    loss, outputs = self(input_ids = input_ids,\n",
        "                         attention_mask = attention_mask,\n",
        "                         decoder_attention_mask = labels_attention_mask,\n",
        "                         labels = labels)\n",
        "    \n",
        "    return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_size):\n",
        "      input_ids = batch[\"text_input_ids\"]\n",
        "      attention_mask = batch[\"text_attention_mask\"]\n",
        "      labels = batch[\"labels\"]\n",
        "      labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "      loss, outputs = self(input_ids = input_ids,\n",
        "                          attention_mask = attention_mask,\n",
        "                          decoder_attention_mask = labels_attention_mask,\n",
        "                          labels = labels)\n",
        "      return loss\n",
        "\n",
        "    def test_step(self, batch, batch_size):\n",
        "      input_ids = batch[\"text_input_ids\"]\n",
        "      attention_mask = batch[\"text_attention_mask\"]\n",
        "      labels = batch[\"labels\"]\n",
        "      labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "      loss, outputs = self(input_ids = input_ids,\n",
        "                          attention_mask = attention_mask,\n",
        "                          decoder_attention_mask = labels_attention_mask,\n",
        "                          labels = labels)\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return AdamW(self.parameters(), lr = 0.0001)"
      ],
      "metadata": {
        "id": "lEbXTAuBjP-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "BATCH_SIZE = 2"
      ],
      "metadata": {
        "id": "Lj8QRxWPjlI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_module = ClickbaitSummaryDataModule(train_df, test_df, tokenizer, BATCH_SIZE)\n",
        "data_module.train_df = train_df\n",
        "data_module.test_df = test_df\n",
        "data_module.tokenizer = tokenizer\n",
        "data_module.text_max_token_len = 512\n",
        "data_module.summary_max_token_len = 128\n",
        "data_module.batch_size = BATCH_SIZE"
      ],
      "metadata": {
        "id": "NZnuI9QQjnc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClickbaitSummaryModel()"
      ],
      "metadata": {
        "id": "Snto15GxjWdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    gpus= 1,\n",
        "    max_epochs = N_EPOCHS,\n",
        "    progress_bar_refresh_rate = 40\n",
        ")"
      ],
      "metadata": {
        "id": "VDHp2Z6Wjd0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "Cjy6U9H9j1rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache() \n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "etlvqbQlj4m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, data_module)"
      ],
      "metadata": {
        "id": "_b3tlegGj60k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = ClickbaitSummaryModel.load_from_checkpoint(\n",
        "    trainer.checkpoint_callback.best_model_path\n",
        ")\n",
        "\n",
        "trained_model.freeze()"
      ],
      "metadata": {
        "id": "2R4JUWXtj_bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarizeText(text):\n",
        "  text_encoding = tokenizer(\n",
        "      text, \n",
        "      max_length = 512,\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      return_attention_mask = True,\n",
        "      add_special_tokens = True,\n",
        "      return_tensors = 'pt'\n",
        "  )\n",
        "\n",
        "  generated_ids = trained_model.model.generate(\n",
        "      input_ids = text_encoding['input_ids'],\n",
        "      attention_mask = text_encoding['attention_mask'],\n",
        "      max_length = 150,\n",
        "      num_beams = 3,\n",
        "      repetition_penalty=2.5,\n",
        "      length_penalty = 1.0,\n",
        "      early_stopping=True\n",
        "  )\n",
        "\n",
        "\n",
        "  preds = [ tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for gen_id in generated_ids\n",
        "  ]\n",
        "  \n",
        "  return \"\".join(preds)"
      ],
      "metadata": {
        "id": "Q_Ufd4DDkB0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_len= len(test_df)"
      ],
      "metadata": {
        "id": "8G_lyd3NkGrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_predictions = [{\"id\": i, \"prediction_text\": summarizeText(context)} for i, context in enumerate(test_df[\"context\"])]\n",
        "references = [{\"id\": i, \"answers\": {\"answer_start\":[0], \"text\":[ex]}} for i, ex in enumerate(test_df[\"answers\"])]\n",
        "result = metric.compute(predictions=formatted_predictions, references=references)"
      ],
      "metadata": {
        "id": "zIIGtY7bkTDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "bleu = evaluate.load('bleu')\n",
        "results = 0\n",
        "prediction_test = []\n",
        "reference_test = []\n",
        "\n",
        "for i in range(test_dataset_len):\n",
        "  prediction_test.append(formatted_predictions[i][\"prediction_text\"].strip())\n",
        "  reference_test.append(references[i][\"answers\"][\"text\"][0].strip())\n",
        "\n",
        "results = bleu.compute(predictions = prediction_test, references = reference_test).get(\"bleu\")\n",
        "\n",
        "print(f\"BLEU SCORE: {results}\")"
      ],
      "metadata": {
        "id": "D2Vnb0iPkXlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = evaluate.load('meteor')\n",
        "results = 0\n",
        "\n",
        "results = meteor.compute(predictions = prediction_test, references = reference_test).get(\"meteor\")\n",
        "\n",
        "print(f\"METEOR SCORE: {results}\")"
      ],
      "metadata": {
        "id": "eRZjAQHqkaQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "KTT5n1X2ka6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore\n",
        "from statistics import mean\n",
        "\n",
        "bert_score = evaluate.load(\"bertscore\")\n",
        "\n",
        "results = bert_score.compute(predictions = prediction_test, references = reference_test, lang='en')\n",
        "\n",
        "print(\"BERT F1 Score: \", mean(results.get(\"f1\")))"
      ],
      "metadata": {
        "id": "ubFEhmoSkdHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install rouge_score"
      ],
      "metadata": {
        "id": "BSnJO1eXkfft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "results = rouge.compute(predictions = prediction_test, references = reference_test)\n",
        "\n",
        "\n",
        "print(results.keys())\n",
        "\n",
        "print(f\"ROGUE SCORE (Uni gram): {results['rouge1']}\")\n",
        "print(f\"ROGUE SCORE (Bi gram): {results['rouge2']}\")\n",
        "print(f\"ROUGE SCORE (LCS): {results['rougeL']}\")\n",
        "print(f\"ROGUE SCORE (Lsum): {results['rougeLsum']}\")"
      ],
      "metadata": {
        "id": "Uv-OTMnRkhms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}